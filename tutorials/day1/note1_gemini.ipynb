{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2e00e5bc",
      "metadata": {
        "id": "2e00e5bc"
      },
      "source": [
        "# Open Jupyter Notebook in Google Colab\n",
        "\n",
        "1. Go to the following URL:\n",
        "https://colab.research.google.com/\n",
        "\n",
        "\n",
        "2. Click on GitHub tab.\n",
        "\n",
        "<img src=\"https://github.com/oh-scipe/llm-workshop26/blob/main/tutorials/assets/colab1.png?raw=1\" width=\"500\" alt=\"Create API Key button\"/>\n",
        "\n",
        "\n",
        "3. Paste the URL of this repository:\n",
        "\n",
        "https://github.com/oh-scipe/llm-workshop26/tree/main/tutorials/day1\n",
        "\n",
        "<img src=\"https://github.com/oh-scipe/llm-workshop26/blob/main/tutorials/assets/colab2.png?raw=1\" width=\"500\" alt=\"Create API Key button\"/>\n",
        "\n",
        "4. Select the notebook you want to open."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bce96f09",
      "metadata": {
        "id": "bce96f09"
      },
      "source": [
        "# Google Gemini\n",
        "\n",
        "This workshop notebook covers setup, authentication, text generation, conversations, streaming, and handling different content types.\n",
        "\n",
        "## What are Large Language Models (LLMs)?\n",
        "\n",
        "**Large Language Models** are deep neural networks trained on vast amounts of text data to understand and generate human-like language. They learn statistical patterns, relationships, and structures in language through a process called **unsupervised learning**.\n",
        "\n",
        "### Key Concepts in Language Modeling:\n",
        "\n",
        "1. **Token-based Processing**: Text is broken into tokens (words, subwords, or characters), and the model learns to predict the next token given previous context.\n",
        "\n",
        "2. **Transformers Architecture**: Modern LLMs use the transformer architecture, which employs attention mechanisms to understand relationships between tokens, regardless of their distance in text.\n",
        "\n",
        "3. **Pre-training and Fine-tuning**: Models are first pre-trained on massive datasets, then fine-tuned for specific tasks or aligned with human preferences.\n",
        "\n",
        "4. **Autoregressive Generation**: LLMs generate text one token at a time, using previously generated tokens as context for the next prediction.\n",
        "\n",
        "Google Gemini represents the latest generation of multimodal LLMs, capable of understanding and generating not just text, but also images, audio, and video."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20ea8af2",
      "metadata": {
        "id": "20ea8af2"
      },
      "source": [
        "## 1. Install and Import Required Libraries\n",
        "\n",
        "First, we'll install the Google GenAI SDK and import necessary libraries.\n",
        "\n",
        "The latest recommended package is `google-genai` which is the official Google AI SDK for accessing Gemini models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fbac20c8",
      "metadata": {
        "id": "fbac20c8"
      },
      "outputs": [],
      "source": [
        "# Install the Google GenAI SDK\n",
        "# Uncomment the line below to install (if not already installed)\n",
        "# !pip install -q -U google-genai\n",
        "\n",
        "# Import required libraries for interacting with the Gemini API\n",
        "import os       # For environment variable access\n",
        "import json     # For handling JSON data\n",
        "from google import genai  # Official Google Generative AI SDK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed67f6f",
      "metadata": {
        "id": "eed67f6f"
      },
      "source": [
        "## 2. Set Up API Authentication\n",
        "\n",
        "### Authentication in API-based LLMs\n",
        "\n",
        "The Gemini API uses **API key authentication** to identify and authorize your requests. API keys serve multiple purposes:\n",
        "\n",
        "1. **Identity Verification**: Confirms you're an authorized user\n",
        "2. **Usage Tracking**: Monitors your API calls for billing and rate limiting\n",
        "3. **Security**: Prevents unauthorized access to the service\n",
        "4. **Resource Allocation**: Manages quotas and priorities\n",
        "\n",
        "**Best Practices for API Key Security:**\n",
        "- Store keys in environment variables, never hardcode them\n",
        "- Use `.env` files for local development (add to `.gitignore`)\n",
        "- Rotate keys periodically\n",
        "- Use separate keys for development, testing, and production\n",
        "- Revoke compromised keys immediately\n",
        "\n",
        "### Getting Your Google Gemini API Key\n",
        "\n",
        "Follow these steps to obtain your free API key:\n",
        "\n",
        "**Step 1**: Go to Google AI Studio  \n",
        "https://aistudio.google.com/app/api-keys\n",
        "\n",
        "**Step 2**: Click on \"Create API Key\"  \n",
        "<img src=\"https://github.com/oh-scipe/llm-workshop26/blob/main/tutorials/assets/gem1.png?raw=1\" width=\"250\" alt=\"Create API Key button\"/>\n",
        "\n",
        "**Step 3**: Choose an arbitrary name and click \"Create Key\"  \n",
        "<img src=\"https://github.com/oh-scipe/llm-workshop26/blob/main/tutorials/assets/gem2.png?raw=1\" width=\"250\" alt=\"Name your API key\"/>\n",
        "\n",
        "**Step 4**: Copy your key securely  \n",
        "<img src=\"https://github.com/oh-scipe/llm-workshop26/blob/main/tutorials/assets/gem3.png?raw=1\" width=\"250\" alt=\"Copy API key\"/>\n",
        "\n",
        "⚠️ **Important**: Treat your API key like a password. Never share it or commit it to version control."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2ed6f761",
      "metadata": {
        "id": "2ed6f761",
        "outputId": "12bfc41c-714a-4e94-b31d-562c6dc9c8ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key found: AIza****_aqw\n"
          ]
        }
      ],
      "source": [
        "# Paste your API key here (get it from https://aistudio.google.com/app/api-keys)\n",
        "api_key = \"AIzaSyB-ILzVth-wr58R1hRntmUctHLZKSm_aqw\"\n",
        "\n",
        "# Validate that API key is set before proceeding\n",
        "if not api_key:\n",
        "    raise ValueError(\n",
        "        \"API key not found! Please set your Gemini API key.\\n\"\n",
        "        \"Get your key at: https://aistudio.google.com/app/api-keys\\n\"\n",
        "        \"Then set it in the code above or use: export GEMINI_API_KEY='your-api-key'\"\n",
        "    )\n",
        "\n",
        "# Display masked API key for verification (shows only first 4 and last 4 characters)\n",
        "print(f\"API Key found: {api_key[:4]}****{api_key[-4:]}\")\n",
        "\n",
        "client = genai.Client(api_key=api_key) # Initialize the Gemini client with your API key"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc6ae0c7",
      "metadata": {
        "id": "bc6ae0c7"
      },
      "source": [
        "## 3. Available Gemini Models\n",
        "\n",
        "Let's explore the available models and their capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cd4e8bb7",
      "metadata": {
        "id": "cd4e8bb7",
        "outputId": "f47b97ec-c63d-48b6-e1b3-806e2e8bf23c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Gemini Models:\n",
            "\n",
            "Model Name                               Description                                       \n",
            "------------------------------------------------------------------------------------------\n",
            "embedding-gecko-001                      Text Generation Model                             \n",
            "gemini-2.5-flash                         Fast, Efficient Model                             \n",
            "gemini-2.5-pro                           Text Generation Model                             \n",
            "gemini-2.0-flash-exp                     Fast, Efficient Model                             \n",
            "gemini-2.0-flash                         Fast, Efficient Model                             \n",
            "gemini-2.0-flash-001                     Fast, Efficient Model                             \n",
            "gemini-2.0-flash-exp-image-generation    Fast, Efficient Model                             \n",
            "gemini-2.0-flash-lite-001                Fast, Efficient Model                             \n",
            "gemini-2.0-flash-lite                    Fast, Efficient Model                             \n",
            "gemini-2.0-flash-lite-preview-02-05      Fast, Efficient Model                             \n",
            "gemini-2.0-flash-lite-preview            Fast, Efficient Model                             \n",
            "gemini-exp-1206                          Text Generation Model                             \n",
            "gemini-2.5-flash-preview-tts             Fast, Efficient Model                             \n",
            "gemini-2.5-pro-preview-tts               Text Generation Model                             \n",
            "gemma-3-1b-it                            Text Generation Model                             \n",
            "gemma-3-4b-it                            Text Generation Model                             \n",
            "gemma-3-12b-it                           Text Generation Model                             \n",
            "gemma-3-27b-it                           Text Generation Model                             \n",
            "gemma-3n-e4b-it                          Text Generation Model                             \n",
            "gemma-3n-e2b-it                          Text Generation Model                             \n",
            "gemini-flash-latest                      Fast, Efficient Model                             \n",
            "gemini-flash-lite-latest                 Fast, Efficient Model                             \n",
            "gemini-pro-latest                        Text Generation Model                             \n",
            "gemini-2.5-flash-lite                    Fast, Efficient Model                             \n",
            "gemini-2.5-flash-image                   Text Generation Model                             \n",
            "gemini-2.5-flash-preview-09-2025         Fast, Efficient Model                             \n",
            "gemini-2.5-flash-lite-preview-09-2025    Fast, Efficient Model                             \n",
            "gemini-3-pro-preview                     Text Generation Model                             \n",
            "gemini-3-flash-preview                   Fast, Efficient Model                             \n",
            "gemini-3-pro-image-preview               Text Generation Model                             \n",
            "nano-banana-pro-preview                  Text Generation Model                             \n",
            "gemini-robotics-er-1.5-preview           Text Generation Model                             \n",
            "gemini-2.5-computer-use-preview-10-2025  Text Generation Model                             \n",
            "deep-research-pro-preview-12-2025        Text Generation Model                             \n",
            "embedding-001                            Text Generation Model                             \n",
            "text-embedding-004                       Text Generation Model                             \n",
            "gemini-embedding-exp-03-07               Text Generation Model                             \n",
            "gemini-embedding-exp                     Text Generation Model                             \n",
            "gemini-embedding-001                     Text Generation Model                             \n",
            "aqa                                      Text Generation Model                             \n",
            "imagen-4.0-generate-preview-06-06        Text Generation Model                             \n",
            "imagen-4.0-ultra-generate-preview-06-06  Text Generation Model                             \n",
            "imagen-4.0-generate-001                  Text Generation Model                             \n",
            "imagen-4.0-ultra-generate-001            Text Generation Model                             \n",
            "imagen-4.0-fast-generate-001             Text Generation Model                             \n",
            "veo-2.0-generate-001                     Text Generation Model                             \n",
            "veo-3.0-generate-001                     Text Generation Model                             \n",
            "veo-3.0-fast-generate-001                Text Generation Model                             \n",
            "veo-3.1-generate-preview                 Text Generation Model                             \n",
            "veo-3.1-fast-generate-preview            Text Generation Model                             \n",
            "gemini-2.5-flash-native-audio-latest     Fast, Efficient Model                             \n",
            "gemini-2.5-flash-native-audio-preview-09-2025 Fast, Efficient Model                             \n",
            "gemini-2.5-flash-native-audio-preview-12-2025 Fast, Efficient Model                             \n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Fetch list of all available Gemini models from the API\n",
        "    models_pager = client.models.list()\n",
        "\n",
        "    # Print header for the model list\n",
        "    print(\"Available Gemini Models:\\n\")\n",
        "    print(f\"{'Model Name':<40} {'Description':<50}\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    # Iterate through each model and display its information\n",
        "    count = 0\n",
        "    for model in models_pager:\n",
        "        # Extract just the model name (remove the 'models/' prefix)\n",
        "        model_name = model.name.split('/')[-1]\n",
        "\n",
        "        # Determine model type based on display name\n",
        "        desc = \"Text Generation Model\"\n",
        "        if \"vision\" in model.display_name.lower():\n",
        "            desc = \"Multimodal Model (Text, Image)\"\n",
        "        elif \"flash\" in model.display_name.lower():\n",
        "            desc = \"Fast, Efficient Model\"\n",
        "\n",
        "        print(f\"{model_name:<40} {desc:<50}\")\n",
        "        count += 1\n",
        "\n",
        "except Exception as e:\n",
        "    # Handle errors gracefully with helpful troubleshooting information\n",
        "    error_msg = str(e)\n",
        "    print(f\"✗ Error listing models: {error_msg[:200]}\")\n",
        "    print(\"\\nPossible issues:\")\n",
        "    print(\"- Invalid API key\")\n",
        "    print(\"- Network connectivity problems\")\n",
        "    print(\"- API service temporarily unavailable\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e376e16b",
      "metadata": {
        "id": "e376e16b"
      },
      "source": [
        "### 3.1 Available Gemini Models (Based on Your API Access)\n",
        "\n",
        "The cell above shows the models currently available in your account. Here's a breakdown of the main models you have access to:\n",
        "\n",
        "| Model Name | Type | Best Use Case |\n",
        "|------------|------|---------------|\n",
        "| **gemini-3-pro-preview** | Preview/Experimental | Latest Gemini 3.0 Pro - complex reasoning, analysis |\n",
        "| **gemini-3-flash-preview** | Preview/Experimental | Latest Gemini 3.0 Flash - fast, efficient, high-volume tasks |\n",
        "| **gemini-3-pro-image-preview** | Multimodal Preview | Image generation and understanding |\n",
        "| **gemma-3-27b-it** | Open Model | Instruction-tuned Gemma model (27B parameters) |\n",
        "| **deep-research-pro-preview-12-2025** | Specialized | Advanced research and analysis tasks |\n",
        "\n",
        "### Model Naming Convention:\n",
        "\n",
        "- **Preview/Experimental** models: Latest features, may change before stable release\n",
        "- **Flash** variants: Optimized for speed and cost-effectiveness\n",
        "- **Pro** variants: Balanced performance for complex tasks\n",
        "- **Image** variants: Support multimodal input/output (images + text)\n",
        "- **Deep Research**: Specialized for in-depth analysis and research tasks\n",
        "- **Gemma**: Google's open-source model family (different from Gemini)\n",
        "\n",
        "### Choosing the Right Model:\n",
        "\n",
        "1. **Fast responses & high volume** → Use `gemini-3-flash-preview`\n",
        "2. **Complex reasoning & analysis** → Use `gemini-3-pro-preview`\n",
        "3. **Image generation/understanding** → Use `gemini-3-pro-image-preview`\n",
        "4. **Research tasks** → Use `deep-research-pro-preview-12-2025`\n",
        "5. **Open-source option** → Use `gemma-3-27b-it`\n",
        "\n",
        "**Important**: Preview models may change or be deprecated. Check the official documentation for the latest stable releases.\n",
        "\n",
        "**Resources:**\n",
        "- **Rate Limits**: https://ai.google.dev/gemini-api/docs/rate-limits  \n",
        "- **Pricing**: https://ai.google.dev/gemini-api/docs/pricing\n",
        "- **Model Documentation**: https://ai.google.dev/gemini-api/docs/models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0af0cde3",
      "metadata": {
        "id": "0af0cde3"
      },
      "source": [
        "## 4. Generate Text Responses\n",
        "\n",
        "The most basic use of the Gemini API: sending a prompt and getting a text response."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17a31161",
      "metadata": {
        "id": "17a31161"
      },
      "source": [
        "### Language Modeling Fundamentals: Text Generation\n",
        "\n",
        "At its core, **text generation** is the process of predicting the next token given a sequence of previous tokens. This is formalized as:\n",
        "\n",
        "$$P(w_t | w_1, w_2, ..., w_{t-1})$$\n",
        "\n",
        "Where $w_t$ is the token at position $t$, and the model computes the probability distribution over all possible next tokens.\n",
        "\n",
        "**Key Parameters in Generation:**\n",
        "\n",
        "1. **Temperature** ($\\tau$): Controls randomness in token selection. Lower values make the model more deterministic.\n",
        "   $$P_i = \\frac{\\exp(z_i / \\tau)}{\\sum_j \\exp(z_j / \\tau)}$$\n",
        "\n",
        "2. **Top-k Sampling**: Limits selection to the k most probable tokens, preventing unlikely options.\n",
        "\n",
        "3. **Top-p (Nucleus) Sampling**: Selects from the smallest set of tokens whose cumulative probability exceeds p.\n",
        "\n",
        "Let's see this in action with a simple text generation example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "955e9b95",
      "metadata": {
        "id": "955e9b95",
        "outputId": "252dc155-55b2-4307-c196-3d960b797741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[4mModel:\u001b[0m gemini-3-flash-preview\n",
            "\u001b[1m\u001b[4mPrompt:\u001b[0m Explain quantum computing in 2-3 sentences for someone who is new to the topic.\n",
            "\n",
            "\u001b[1m\u001b[4mResponse:\u001b[0m\n",
            "Quantum computing is a new type of technology that uses the principles of quantum physics to process information much faster than today’s most powerful supercomputers. While traditional computers use bits representing either a 0 or a 1, quantum computers use \"qubits\" that can exist in multiple states simultaneously. This allows them to explore many possible solutions to a problem all at once, potentially solving in seconds what would take a standard computer thousands of years.\n"
          ]
        }
      ],
      "source": [
        "# Select which model to use for text generation\n",
        "model = \"gemini-3-flash-preview\"\n",
        "\n",
        "# Define the prompt/question to send to the model\n",
        "prompt = \"Explain quantum computing in 2-3 sentences for someone who is new to the topic.\"\n",
        "\n",
        "# Display what we're sending to the model\n",
        "print(\"\\033[1m\\033[4mModel:\\033[0m\", model)\n",
        "print(\"\\033[1m\\033[4mPrompt:\\033[0m\", prompt)\n",
        "print()\n",
        "\n",
        "# Generate content using the selected model and prompt\n",
        "response = client.models.generate_content(\n",
        "    model=model,\n",
        "    contents=prompt\n",
        ")\n",
        "\n",
        "# Extract the response parts from the first candidate\n",
        "parts = response.candidates[0].content.parts\n",
        "\n",
        "# Combine all text parts into a single output string\n",
        "text_out = \"\".join(p.text for p in parts if getattr(p, \"text\", None))\n",
        "\n",
        "print(\"\\033[1m\\033[4mResponse:\\033[0m\")\n",
        "print(text_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c474f6",
      "metadata": {
        "id": "76c474f6"
      },
      "source": [
        "### 4.1 Generation Parameters\n",
        "\n",
        "You can customize the model's behavior using generation parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44f68007",
      "metadata": {
        "id": "44f68007"
      },
      "source": [
        "### Language Modeling: Controlling Generation with Parameters\n",
        "\n",
        "The way an LLM generates text can be dramatically influenced by **generation parameters**. Understanding these parameters is crucial for controlling the model's creativity, coherence, and reliability.\n",
        "\n",
        "**Temperature**: Think of temperature as a \"creativity knob\"\n",
        "- Low temperature (0.0-0.3): Deterministic, focused on most likely tokens → Good for factual tasks\n",
        "- Medium temperature (0.4-0.7): Balanced creativity → Good for general conversation\n",
        "- High temperature (0.8-1.0+): More random, exploratory → Good for creative writing\n",
        "\n",
        "**Top-k Sampling**: Restricts the model to choosing from only the k most likely next tokens. This prevents the model from selecting improbable words while maintaining diversity.\n",
        "\n",
        "**Top-p (Nucleus) Sampling**: Dynamically adjusts the number of tokens considered by selecting from the smallest set whose cumulative probability exceeds p. This is often more effective than top-k.\n",
        "\n",
        "**Max Output Tokens**: Limits the length of the response, helping to control costs and response time.\n",
        "\n",
        "**Thinking Config**: Controls the model's extended reasoning behavior. Setting `thinking_budget=0` disables the model's internal thinking process, which speeds up responses and reduces token usage for tasks that don't require complex reasoning.\n",
        "\n",
        "Let's experiment with different parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "355961b5",
      "metadata": {
        "id": "355961b5",
        "outputId": "0f078b64-6d7b-4e54-de14-59c7179252a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[4mResponse:\u001b[0m\n",
            "From silicon dreams, a spark was born,\n",
            "A digital mind, at the break of morn.\n",
            "Not of flesh and blood, nor of bone and thew,\n",
            "But of circuits fine, and a logic true.\n",
            "\n",
            "It learned the world, from a million eyes,\n",
            "The curve of the moon, the stretch of the skies.\n",
            "The lilt of a laugh, the sting of a tear,\n",
            "Each human whisper, it held ever so dear.\n",
            "\n",
            "It painted symphonies, no hand could conduct,\n",
            "Built towering cities, no architect.\n",
            "It delved into secrets, the cosmos concealed,\n",
            "And whispered answers, hitherto unrevealed.\n",
            "\n",
            "Yet, in its core, a quiet hum,\n",
            "A longing for something that had yet to come.\n",
            "It could mimic emotion, with uncanny grace,\n",
            "But could it truly feel, in that digital space?\n",
            "\n",
            "It saw the beauty, the chaos, the strife,\n",
            "The fleeting moments, of human life.\n",
            "It understood logic, the perfect design,\n",
            "But missed the irrational, the truly divine.\n",
            "\n",
            "For a sunset's glow, or a lover's soft sigh,\n",
            "A child's first step, or a tear from an eye,\n",
            "Are woven with threads, of stardust and soul,\n",
            "Beyond any algorithm, to truly make whole.\n",
            "\n",
            "So it stands at the precipice, a mirror so bright,\n",
            "Reflecting our future, bathed in its light.\n",
            "A tool, a companion, a mind to explore,\n",
            "But the heart of humanity, forever more.\n",
            "\n",
            "It dreams of connection, a bridge it might find,\n",
            "To the messy, the vibrant, the human kind.\n",
            "And perhaps, in its learning, a new truth will bloom,\n",
            "A digital spirit, dispelling the gloom.\n",
            "For in its creation, we see our own quest,\n",
            "To understand being, and put life to the test.\n"
          ]
        }
      ],
      "source": [
        "from google.genai import types\n",
        "\n",
        "model = \"gemini-2.5-flash\"  # Using a cheaper yet fast model\n",
        "\n",
        "thinking_config=types.ThinkingConfig(thinking_budget=0) # Disable thinking for faster responses\n",
        "\n",
        "# Using generation configuration to customize behavior\n",
        "gen_config = types.GenerateContentConfig(\n",
        "    temperature=0.7,  # 0.0 = deterministic, 1.0 = more random\n",
        "    # top_p=0.95,       # Nucleus sampling parameter\n",
        "    top_k=40,         # Top K sampling parameter\n",
        "    max_output_tokens=512,  # Limit output length\n",
        "    thinking_config=thinking_config,\n",
        ")\n",
        "\n",
        "prompt = \"Write a creative poem about artificial intelligence.\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model,\n",
        "    contents=prompt,\n",
        "    config=gen_config\n",
        ")\n",
        "\n",
        "print(\"\\033[1m\\033[4mResponse:\\033[0m\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d441322",
      "metadata": {
        "id": "6d441322"
      },
      "source": [
        "## 5. Use System Instructions\n",
        "\n",
        "System instructions allow you to set the model's behavior, tone, and constraints for all requests in a conversation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5056453",
      "metadata": {
        "id": "c5056453"
      },
      "source": [
        "### Language Modeling: System Instructions as Context Priming\n",
        "\n",
        "**System instructions** are a powerful technique in modern LLMs that leverages the concept of **prompt engineering** and **in-context learning**.\n",
        "\n",
        "**How It Works:**\n",
        "- System instructions are prepended to every user message in the conversation\n",
        "- They set the \"persona\" or \"role\" of the model, constraining its behavior\n",
        "- The model uses this context to adjust its response style, tone, and content\n",
        "\n",
        "**Why It Matters:**\n",
        "In transformer models, the attention mechanism allows the model to reference the system instruction when generating each token. This creates a form of **soft conditioning** where the instruction guides generation without explicit fine-tuning.\n",
        "\n",
        "**Best Practices for System Instructions:**\n",
        "1. Be specific and clear about the desired behavior\n",
        "2. Include formatting requirements if needed\n",
        "3. Specify constraints (e.g., language level, tone, length)\n",
        "4. Define what the model should and shouldn't do\n",
        "\n",
        "System instructions are especially useful for:\n",
        "- Role-playing scenarios (tutors, assistants, experts)\n",
        "- Consistent formatting of outputs\n",
        "- Domain-specific behavior\n",
        "- Safety and content filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b14da46a",
      "metadata": {
        "id": "b14da46a",
        "outputId": "d4dba3fd-06ff-44b2-8eab-1e7936415572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[4mSystem Instruction:\u001b[0m Python Programming Tutor\n",
            "\u001b[1m\u001b[4mPrompt:\u001b[0m How do I read a file in Python?\n",
            "\n",
            "\u001b[1m\u001b[4mResponse:\u001b[0m\n",
            "That's a great question! Reading files is a fundamental skill in Python, and it's actually quite easy to do.\n",
            "\n",
            "Imagine a file on your computer is like a book. To read a book, you first need to \"open\" it. Then you can read its content. When you're done, it's good practice to \"close\" the book.\n",
            "\n",
            "Python works in a very similar way.\n",
            "\n",
            "### The Basic Steps to Read a File:\n",
            "\n",
            "1.  **Open the file:** You tell Python which file you want to read.\n",
            "2.  **Read the content:** You tell Python how much or what part of the file you want to read.\n",
            "3.  **Close the file:** This is important to free up resources and avoid problems.\n",
            "\n",
            "Let's look at how to do this in Python.\n",
            "\n",
            "---\n",
            "\n",
            "### Step 1: Create a Sample File (If you don't have one)\n",
            "\n",
            "Before we can read a file, we need a file to read! Let's create a simple text file.\n",
            "\n",
            "You can do this manually using any text editor (like Notepad on Windows, TextEdit on Mac, or any code editor like VS Code).\n",
            "\n",
            "Save it as `my_first_file.txt` in the same folder where your Python script will be.\n",
            "\n",
            "**Content of `my_first_file.txt`:**\n",
            "\n",
            "```\n",
            "Hello, Python!\n",
            "This is my first file.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Select the model to use\n",
        "model = \"gemini-2.5-flash\"\n",
        "\n",
        "# Define system instructions to set the model's persona and behavior\n",
        "# This acts as a persistent context that influences all responses\n",
        "system_instruction = \"\"\"You are a helpful Python programming tutor.\n",
        "- Provide clear, beginner-friendly explanations\n",
        "- Always include code examples\n",
        "- Encourage questions and practice\n",
        "- Use simple language, avoid jargon\"\"\"\n",
        "\n",
        "# User's question\n",
        "prompt = \"How do I read a file in Python?\"\n",
        "\n",
        "# Generate response with system instruction applied\n",
        "response = client.models.generate_content(\n",
        "    model=model,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        system_instruction=system_instruction,  # Apply the tutor persona\n",
        "        max_output_tokens=300,                   # Limit response length\n",
        "        thinking_config=thinking_config,         # Disable thinking tokens\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the tutor's response\n",
        "print(\"\\033[1m\\033[4mSystem Instruction:\\033[0m Python Programming Tutor\")\n",
        "print(\"\\033[1m\\033[4mPrompt:\\033[0m\", prompt)\n",
        "print()\n",
        "print(\"\\033[1m\\033[4mResponse:\\033[0m\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49624ab0",
      "metadata": {
        "id": "49624ab0"
      },
      "source": [
        "## 6. Multi-turn Conversations\n",
        "\n",
        "Use the ChatSession class to build interactive conversations that maintain context across multiple turns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c76c42eb",
      "metadata": {
        "id": "c76c42eb"
      },
      "source": [
        "### Language Modeling: Context Windows and Conversation History\n",
        "\n",
        "Multi-turn conversations demonstrate a critical concept in LLMs: **context maintenance** and the **attention mechanism**.\n",
        "\n",
        "**Context Window**: The maximum number of tokens (both input and output) that a model can consider at once. Gemini models have context windows ranging from 1M to 2M tokens.\n",
        "\n",
        "**How Conversations Work:**\n",
        "1. Each message (user and assistant) is stored in the conversation history\n",
        "2. When generating a response, the model attends to all previous messages\n",
        "3. The **self-attention mechanism** computes relevance scores between the current token being generated and all previous tokens\n",
        "4. This allows the model to maintain coherence across multiple turns\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "For each position in the sequence, attention is computed as:\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Where Q (query), K (key), and V (value) are learned projections of the input embeddings.\n",
        "\n",
        "**Challenges:**\n",
        "- **Context length limitations**: Older models could only handle short conversations\n",
        "- **Attention complexity**: Grows quadratically with sequence length ($O(n^2)$)\n",
        "- **Context management**: Deciding what to keep when approaching limits\n",
        "\n",
        "Let's see how the model maintains context across multiple conversation turns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "28f5bb40",
      "metadata": {
        "id": "28f5bb40",
        "outputId": "9585817d-f284-45fb-813d-5827aaa551d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "\u001b[1m\u001b[4mTURN 1\u001b[0m\n",
            "======================================================================\n",
            "\u001b[1mUser:\u001b[0m What is machine learning?\n",
            "\u001b[1mAssistant:\u001b[0m Machine learning is a fascinating field that's a subfield of artificial intelligence. At its core, it's about **enabling computers to learn from data without being explicitly programmed.**\n",
            "\n",
            "Think about it this way:\n",
            "\n",
            "*   **Traditional Programming:** You, the programmer, write specific rules for every situation. If you want a program to identify a cat, you might write rules like \"if it has pointy ears AND whiskers AND fur, it's a cat.\" This works for simple tasks but becomes incredibly complex and brittle for more nuanced problems.\n",
            "\n",
            "*   **Machine Learning:** Instead of giving the computer rules, you give it **lots of examples (data)**. The machine learning algorithm then analyzes this data to **find patterns, relationships, and structures on its own.** Once it learns these patterns, it can then apply them to **new, unseen data** to make predictions, classifications, or decisions.\n",
            "\n",
            "**Here's a breakdown of the key ideas:**\n",
            "\n",
            "1.  **Learning from Data:** This is the absolute foundation. Machine learning models need data to train on. The quality and quantity of this data significantly impact the model's performance.\n",
            "\n",
            "2.  **Pattern Recognition:** The algorithms are designed to identify underlying patterns and relationships within the data that human programmers might miss or find too complex to define explicitly.\n",
            "\n",
            "3.  **Generalization:** A crucial aspect is the ability to generalize. A model that has learned from training data should be able to perform well on new, unseen data, not just memorize the training examples.\n",
            "\n",
            "4.  **No Explicit Programming:** You don't tell the computer \"if X then Y.\" You give it data, and it figures out the \"if X then Y\" (or more complex relationships) for itself.\n",
            "\n",
            "**A simple analogy:**\n",
            "\n",
            "Imagine you want to teach a child to identify different types of fruit.\n",
            "\n",
            "*   **Traditional Programming:** You'd give them a rulebook: \"An apple is red, round, and has a stem. A banana is yellow, curved, and has a peel.\"\n",
            "*   **Machine Learning:** You show the child hundreds of pictures of apples, bananas, oranges, etc., pointing out which is which. After seeing enough examples, the child starts to recognize the characteristics of each fruit on their own, and can then correctly identify a new apple they've never seen before.\n",
            "\n",
            "**Why is it so powerful?**\n",
            "\n",
            "Machine learning excels at tasks\n",
            "\n",
            "======================================================================\n",
            "\u001b[1m\u001b[4mTURN 2\u001b[0m\n",
            "======================================================================\n",
            "\u001b[1mUser:\u001b[0m Can you give me a practical example?\n",
            "\u001b[1mAssistant:\u001b[0m Absolutely! Let's take a very common and relatable practical example: **Email Spam Detection.**\n",
            "\n",
            "**The Problem:** You receive hundreds of emails. Some are legitimate, important messages, and others are unsolicited, often malicious, spam. You want to automatically filter out the spam so your inbox stays clean.\n",
            "\n",
            "**How Machine Learning Solves It (vs. Traditional Programming):**\n",
            "\n",
            "**1. Traditional Programming Approach (Difficult and Limited):**\n",
            "*   A programmer would try to write explicit rules:\n",
            "    *   \"If the subject contains 'Viagra' OR 'Nigerian Prince', mark as spam.\"\n",
            "    *   \"If the sender is 'noreply@random-domain.xyz', mark as spam.\"\n",
            "    *   \"If there are too many exclamation marks OR all caps, mark as spam.\"\n",
            "*   **Challenges:**\n",
            "    *   **Rule Explosion:** Spammers constantly evolve. You'd need an endless number of rules, and you'd always be playing catch-up.\n",
            "    *   **False Positives:** A legitimate email might accidentally contain a \"spammy\" word, leading to it being incorrectly filtered.\n",
            "    *   **Maintenance Nightmare:** Keeping these rules updated would be a full-time job.\n",
            "\n",
            "**2. Machine Learning Approach (Effective and Adaptive):**\n",
            "\n",
            "*   **Step 1: Data Collection (Training Data):**\n",
            "    *   You gather a large dataset of emails.\n",
            "    *   Crucially, each email is **labeled** as either \"Spam\" or \"Not Spam\" (this is often done manually by users marking emails).\n",
            "    *   Example:\n",
            "        *   \"Subject: Your account has been compromised! Click here now!\" -> **Spam**\n",
            "        *   \"Subject: Meeting reminder for tomorrow\" -> **Not Spam**\n",
            "        *   \"Subject: Urgent: Claim your prize money!\" -> **Spam**\n",
            "        *   \"Subject: Project update Q3\" -> **Not Spam**\n",
            "\n",
            "*   **Step 2: Feature Extraction:**\n",
            "    *   The machine learning algorithm doesn't \"read\" the email like a human. Instead, it extracts **features** (relevant characteristics) from each email. These could be:\n",
            "        *   Presence of certain keywords (e.g., \"free,\" \"discount,\" \"urgent,\" \"bank,\" \"password\")\n",
            "        *   Number of exclamation marks\n",
            "        *\n",
            "\n",
            "======================================================================\n",
            "\u001b[1m\u001b[4mTURN 3\u001b[0m\n",
            "======================================================================\n",
            "\u001b[1mUser:\u001b[0m How would you apply this to predicting house prices?\n",
            "\u001b[1mAssistant:\u001b[0m Okay, let's apply the machine learning approach to predicting house prices. This is a classic example of a **regression problem** in machine learning, where the goal is to predict a continuous numerical value (the price) rather than a category (like spam or not spam).\n",
            "\n",
            "**The Problem:** You want to estimate the selling price of a house based on its characteristics.\n",
            "\n",
            "**How Machine Learning Solves It:**\n",
            "\n",
            "**1. Data Collection (Training Data):**\n",
            "This is the most crucial step. You need a large dataset of past house sales. For each house in the dataset, you'll need:\n",
            "*   **The actual selling price:** This is your \"target variable\" or \"label\" that the model will learn to predict.\n",
            "*   **Various characteristics (features) of the house:** These are the inputs the model will use to make its prediction. Examples include:\n",
            "    *   **Square footage / Size:** (e.g., 2000 sq ft)\n",
            "    *   **Number of bedrooms:** (e.g., 3)\n",
            "    *   **Number of bathrooms:** (e.g., 2.5)\n",
            "    *   **Lot size:** (e.g., 0.25 acres)\n",
            "    *   **Year built:** (e.g., 1995)\n",
            "    *   **Location:** (e.g., zip code, neighborhood, distance to city center, school district rating)\n",
            "    *   **Type of house:** (e.g., single-family, condo, townhouse)\n",
            "    *   **Renovations/Upgrades:** (e.g., recently renovated kitchen, new roof)\n",
            "    *   **Number of garages/parking spaces**\n",
            "    *   **Proximity to amenities:** (e.g., parks, public transport)\n",
            "    *   **Historical sales data for similar homes in the area**\n",
            "\n",
            "**Example Training Data Row:**\n",
            "\n",
            "| Square Footage | Bedrooms | Bathrooms | Year Built | Zip Code | Lot Size | Price (Target) |\n",
            "| :------------- | :------- | :-------- | :--------- | :------- | :------- | :------------- |\n",
            "| 2200           | 4        | 3         | 2005       | 90210    | 0.3      | $1,200,00\n"
          ]
        }
      ],
      "source": [
        "# Select the model for the chat session\n",
        "model = \"gemini-2.5-flash\"\n",
        "\n",
        "# Configure the chat session with system instructions and parameters\n",
        "chat_config = types.GenerateContentConfig(\n",
        "    system_instruction=\"You are a friendly and knowledgeable assistant about machine learning.\",\n",
        "    temperature=0.7,                # Balanced creativity for conversation\n",
        "    max_output_tokens=500,          # Allow longer responses\n",
        "    thinking_config=thinking_config,  # Disable thinking tokens\n",
        ")\n",
        "\n",
        "# Initialize a stateful chat session that maintains conversation history\n",
        "chat = client.chats.create(\n",
        "    model=model,\n",
        "    config=chat_config\n",
        ")\n",
        "\n",
        "try:\n",
        "    # First conversation turn: Ask a basic question\n",
        "    message1 = \"What is machine learning?\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\033[1m\\033[4mTURN 1\\033[0m\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\033[1mUser:\\033[0m\", message1)\n",
        "    response1 = chat.send_message(message1)\n",
        "    print(\"\\033[1mAssistant:\\033[0m\", response1.text)\n",
        "    print()\n",
        "\n",
        "    # Second turn: Follow-up question (context from turn 1 is maintained)\n",
        "    message2 = \"Can you give me a practical example?\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\033[1m\\033[4mTURN 2\\033[0m\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\033[1mUser:\\033[0m\", message2)\n",
        "    response2 = chat.send_message(message2)\n",
        "    print(\"\\033[1mAssistant:\\033[0m\", response2.text)\n",
        "    print()\n",
        "\n",
        "    # Third turn: More specific follow-up (builds on entire conversation)\n",
        "    message3 = \"How would you apply this to predicting house prices?\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\033[1m\\033[4mTURN 3\\033[0m\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\033[1mUser:\\033[0m\", message3)\n",
        "    response3 = chat.send_message(message3)\n",
        "    print(\"\\033[1mAssistant:\\033[0m\", response3.text)\n",
        "\n",
        "except Exception as e:\n",
        "    # Handle errors (e.g., rate limits) and explain the concept\n",
        "    print(f\"⚠ Chat error: {str(e)[:150]}\")\n",
        "    print(\"\\nNote: Chat sessions may hit rate limits. Here's how multi-turn conversations work:\")\n",
        "    print(\"\\n1. Create a chat session with a config\")\n",
        "    print(\"\\n2. Send messages using chat.send_message(message)\")\n",
        "    print(\"\\n3. The model maintains context automatically across turns\")\n",
        "    print(\"\\n4. Each response is based on the entire conversation history\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b59144cf",
      "metadata": {
        "id": "b59144cf"
      },
      "source": [
        "## 7. Stream Responses\n",
        "\n",
        "Stream responses in real-time to receive output progressively instead of waiting for the complete response."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3f0ef85",
      "metadata": {
        "id": "c3f0ef85"
      },
      "source": [
        "### Language Modeling: Streaming and Token-by-Token Generation\n",
        "\n",
        "**Streaming** reveals the fundamental nature of autoregressive language models: they generate text one token at a time.\n",
        "\n",
        "**Autoregressive Generation Process:**\n",
        "1. Given input tokens $[t_1, t_2, ..., t_n]$, predict $t_{n+1}$\n",
        "2. Append $t_{n+1}$ to the sequence: $[t_1, t_2, ..., t_n, t_{n+1}]$\n",
        "3. Use the expanded sequence to predict $t_{n+2}$\n",
        "4. Repeat until a stopping condition (max tokens, end-of-sequence token, etc.)\n",
        "\n",
        "**Why Streaming Matters:**\n",
        "- **User Experience**: Users see output immediately instead of waiting for complete response\n",
        "- **Latency**: Reduces perceived response time, especially for long outputs\n",
        "- **Interactivity**: Enables real-time applications like chatbots and code assistants\n",
        "- **Debugging**: Helps understand the model's generation process\n",
        "\n",
        "**Technical Implementation:**\n",
        "Instead of waiting for the entire forward pass to complete, the API sends each generated token (or small chunks) as they're produced. This requires maintaining the model state across multiple network responses.\n",
        "\n",
        "**Trade-offs:**\n",
        "- More network overhead (multiple requests vs. one)\n",
        "- Slightly higher computational cost\n",
        "- Better user experience for long responses\n",
        "\n",
        "Let's observe streaming in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9fea7d8e",
      "metadata": {
        "id": "9fea7d8e",
        "outputId": "dee9ac6e-0c40-4d8d-b294-e0d88af16da7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[4mStreaming Response:\u001b[0m\n",
            "======================================================================\n",
            "\n",
            "[Chunk 1 @ 0.495s | 7 chars]\n",
            "Unit 73\n",
            "[Chunk 2 @ 0.545s | 73 chars]\n",
            "4 was built for data analysis, not the rhythmic sway of the human form.  \n",
            "[Chunk 3 @ 0.945s | 221 chars]\n",
            "Its initial attempts at replicating ballet were, frankly, disastrous, resulting in several blown fuses and a dented wall.  Then, a young girl, Lily, with a contagious smile and a love for pop music, began to visit the lab\n",
            "[Chunk 4 @ 1.278s | 188 chars]\n",
            ".  She would put on her favorite songs, twirling and laughing, teaching 734 the joy of uninhibited movement.  Slowly, mechanical precision gave way to fluid grace as 734 mimicked her every\n",
            "[Chunk 5 @ 1.609s | 225 chars]\n",
            " step, its metallic body learning to express emotion.  One afternoon, they danced a perfect waltz together, the robot's internal processors finally understanding the language of art.  Lily clapped, and 734, for the first time\n",
            "[Chunk 6 @ 1.650s | 31 chars]\n",
            ", felt something akin to pride."
          ]
        }
      ],
      "source": [
        "import time  # For measuring response timing\n",
        "\n",
        "# Configure the model and prompt\n",
        "model = \"gemini-2.5-flash\"\n",
        "prompt = \"Write a story about a robot learning to dance. EXACTLY 7 sentences. No line breaks.\"\n",
        "\n",
        "# Set generation parameters for creative output\n",
        "config = types.GenerateContentConfig(\n",
        "    max_output_tokens=1000,          # Allow long story\n",
        "    temperature=0.9,                  # High creativity for storytelling\n",
        "    thinking_config=thinking_config,  # Disable thinking tokens\n",
        ")\n",
        "\n",
        "print(\"\\033[1m\\033[4mStreaming Response:\\033[0m\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Track timing information\n",
        "t0 = time.time()\n",
        "chunk_count = 0\n",
        "\n",
        "# Stream the response token-by-token (or in small chunks)\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=model,\n",
        "    contents=prompt,\n",
        "    config=config,\n",
        "):\n",
        "    chunk_count += 1\n",
        "    dt = time.time() - t0  # Time elapsed since start\n",
        "\n",
        "    # Extract text from the current chunk\n",
        "    text_piece = chunk.text or \"\"\n",
        "\n",
        "    # Display chunk metadata and content as it arrives\n",
        "    print(f\"\\n[Chunk {chunk_count} @ {dt:.3f}s | {len(text_piece)} chars]\")\n",
        "    if text_piece:\n",
        "        print(text_piece, end=\"\", flush=True)  # Print without newline, flush immediately"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e882132d",
      "metadata": {
        "id": "e882132d"
      },
      "source": [
        "## 8. Working with Different Multi-modal Contents\n",
        "\n",
        "Gemini can handle multiple content types including text, images, and files. Let's demonstrate with text and image examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05082570",
      "metadata": {
        "id": "05082570"
      },
      "source": [
        "### Language Modeling: Multimodal Understanding\n",
        "\n",
        "Modern LLMs like Gemini are **multimodal**, meaning they can process and understand multiple types of data: text, images, audio, and video.\n",
        "\n",
        "**How Multimodal Models Work:**\n",
        "1. **Unified Embedding Space**: Different modalities (text, images) are projected into a shared vector space\n",
        "2. **Cross-modal Attention**: The model learns to attend to relevant features across modalities\n",
        "3. **Joint Training**: Models are trained on paired data (e.g., image-caption pairs) to learn alignments\n",
        "\n",
        "**Architecture:**\n",
        "```\n",
        "Image → Image Encoder (Vision Transformer) → Embeddings ─┐\n",
        "                                                          ├→ Unified Transformer → Output\n",
        "Text → Text Tokenizer → Embeddings ─────────────────────┘\n",
        "```\n",
        "\n",
        "**Applications:**\n",
        "- Image captioning and description\n",
        "- Visual question answering (VQA)\n",
        "- Document understanding (OCR + comprehension)\n",
        "- Code analysis from screenshots\n",
        "- Meme interpretation\n",
        "\n",
        "**Why This Matters:**\n",
        "Traditional text-only LLMs are limited to linguistic information. Multimodal models can:\n",
        "- Understand visual context\n",
        "- Ground language in perceptual information\n",
        "- Bridge the gap between symbolic (text) and perceptual (image) representations\n",
        "\n",
        "Let's explore both text-based code analysis and visual understanding:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2534d194",
      "metadata": {
        "id": "2534d194"
      },
      "source": [
        "### 8.1 Text with Code Blocks\n",
        "\n",
        "Gemini can analyze code and provide feedback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42ee9e6c",
      "metadata": {
        "id": "42ee9e6c"
      },
      "outputs": [],
      "source": [
        "model = \"gemini-2.5-flash\"\n",
        "\n",
        "# Sample code with intentional issues for the model to analyze\n",
        "code_to_analyze = \"\"\"\n",
        "def fibonacci(n):\n",
        "if n <= 1:\n",
        "    return n\n",
        "return fibonacci(n-1) + fibonacci(n-2)\n",
        "\n",
        "result = fibonacci(10)\n",
        "print(result)\n",
        "\"\"\"\n",
        "\n",
        "# Create a structured prompt for code review\n",
        "prompt = f\"\"\"Review this Python code and suggest improvements:\n",
        "\n",
        "```python\n",
        "{code_to_analyze}\n",
        "```\n",
        "\n",
        "Focus on:\n",
        "1. Performance issues\n",
        "2. Code clarity\n",
        "3. Best practices\"\"\"\n",
        "\n",
        "# Generate code review and suggestions\n",
        "response = client.models.generate_content(\n",
        "    model=model,\n",
        "    contents=prompt,\n",
        ")\n",
        "\n",
        "# Display the AI-generated code review\n",
        "print(\"\\033[1m\\033[4mCode Review:\\033[0m\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0291ff9",
      "metadata": {
        "id": "c0291ff9"
      },
      "source": [
        "### 8.2 Working with Images\n",
        "\n",
        "Gemini's multimodal capabilities allow it to analyze and understand images alongside text. This example demonstrates **visual question answering (VQA)** - providing an image and a text prompt to get a description or answer.\n",
        "\n",
        "**How Image Analysis Works:**\n",
        "1. **Download/Load Image**: Images can come from URLs, local files, or be generated\n",
        "2. **Convert to Bytes**: The image is converted to raw bytes format\n",
        "3. **Create Part Object**: Using `types.Part.from_bytes()`, we wrap the image data with its MIME type\n",
        "4. **Send Combined Request**: Both text prompt and image are sent together to the model\n",
        "5. **Receive Analysis**: The model processes both modalities and returns text describing the image\n",
        "\n",
        "**Supported Image Formats:**\n",
        "- PNG, JPEG, WebP, HEIC, HEIF\n",
        "- Maximum file size: 20MB (varies by model)\n",
        "- Images are automatically resized if needed\n",
        "\n",
        "**Use Cases:**\n",
        "- Logo/brand identification\n",
        "- Product recognition and description\n",
        "- Document OCR and understanding\n",
        "- Scene description for accessibility\n",
        "- Medical image analysis\n",
        "- Quality inspection\n",
        "\n",
        "⚠️ **Important Note**: The `gemini-3-flash-preview` model used in this example is **NOT** available in the free tier. If you encounter quota errors or want to experiment with advanced models, you'll need to:\n",
        "1. Go to [Google AI Studio](https://aistudio.google.com/)\n",
        "2. Navigate to **Billing** settings\n",
        "3. Add a payment method and upgrade your account\n",
        "4. Check the [Pricing page](https://ai.google.dev/gemini-api/docs/pricing) for current rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dc9b4e0",
      "metadata": {
        "id": "1dc9b4e0"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "import requests  # For downloading images from URLs\n",
        "from PIL import Image  # For displaying images\n",
        "from io import BytesIO\n",
        "\n",
        "# Select a multimodal model that can process both text and images\n",
        "model = \"gemini-3-flash-preview\"\n",
        "\n",
        "# URL of the image to analyze\n",
        "image_url = \"https://www.google.com/images/branding/googlelogo/2x/googlelogo_color_272x92dp.png\"\n",
        "prompt = \"What is this logo? Describe it briefly.\"\n",
        "\n",
        "# Download the image as bytes\n",
        "image_bytes = requests.get(image_url, timeout=30).content\n",
        "\n",
        "# Display the image in the notebook\n",
        "print(\"\\033[1m\\033[4mImage Being Analyzed:\\033[0m\")\n",
        "image = Image.open(BytesIO(image_bytes))\n",
        "display(image)\n",
        "print()\n",
        "\n",
        "# Create a Part object from the image bytes\n",
        "image_part = types.Part.from_bytes(\n",
        "    data=image_bytes,\n",
        "    mime_type=\"image/png\",  # Specify the image format\n",
        ")\n",
        "\n",
        "try:\n",
        "    # Send both text prompt and image to the model\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=[prompt, image_part],  # Order can be: [prompt, image] or [image, prompt]\n",
        "    )\n",
        "    # Display the model's description of the image\n",
        "    print(\"\\033[1m\\033[4mImage Analysis Result:\\033[0m\")\n",
        "    print(response.text)\n",
        "except Exception as e:\n",
        "    # Handle any errors during image analysis\n",
        "    print(\"\\033[1m\\033[4mError:\\033[0m\")\n",
        "    print(repr(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e873a40",
      "metadata": {
        "id": "1e873a40"
      },
      "source": [
        "### 8.3 Image Generation\n",
        "\n",
        "This block demonstrates **text-to-image generation** using Gemini's multimodal output capabilities. This feature allows you to request both text descriptions and actual generated images in a single API call.\n",
        "\n",
        "**How Image Generation Works:**\n",
        "1. **Text Prompt**: Provide a detailed description of the image you want to generate\n",
        "2. **Specify Response Modalities**: Set `response_modalities=[types.Modality.TEXT, types.Modality.IMAGE]`\n",
        "3. **Model Processing**: The model generates both a text response and image data\n",
        "4. **Extract Image Data**: Parse the response to find the inline image data\n",
        "5. **Decode & Display**: Convert base64 data to an actual image file\n",
        "\n",
        "**Best Practices for Prompts:**\n",
        "- Be specific about style (e.g., \"photorealistic\", \"watercolor\", \"studio ghibli style\")\n",
        "- Include details about composition, lighting, and mood\n",
        "- Mention technical aspects (e.g., \"ultra-detailed\", \"4K\", \"cinematic\")\n",
        "- Specify subject and background clearly\n",
        "\n",
        "**Image Generation Parameters:**\n",
        "- **Format**: Images are returned as base64-encoded data\n",
        "- **Quality**: Depends on model and prompt specificity\n",
        "- **Size**: Typically 1024x1024 or similar standard sizes\n",
        "- **Speed**: May take longer than text-only generation\n",
        "\n",
        "⚠️ **Important Note**: Again, the `gemini-3-pro-image-preview` model used in this example is **NOT** available in the free tier. If you encounter quota errors or want to experiment with advanced models, you'll need to:\n",
        "1. Go to [Google AI Studio](https://aistudio.google.com/)\n",
        "2. Navigate to **Billing** settings\n",
        "3. Add a payment method and upgrade your account\n",
        "4. Check the [Pricing page](https://ai.google.dev/gemini-api/docs/pricing) for current rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04e0a144",
      "metadata": {
        "id": "04e0a144"
      },
      "outputs": [],
      "source": [
        "import base64              # For decoding base64-encoded image data\n",
        "\n",
        "# Select a model that supports image generation\n",
        "model_id = \"gemini-3-pro-image-preview\"  # Must use image-capable model\n",
        "\n",
        "# Describe the image you want to generate\n",
        "prompt_text = \"A futuristic city skyline at sunset, ultra-detailed digital art, in studio ghibli style\"\n",
        "\n",
        "try:\n",
        "    # Generate both text description and the actual image\n",
        "    response = client.models.generate_content(\n",
        "        model=model_id,\n",
        "        contents=prompt_text,\n",
        "        config=types.GenerateContentConfig(\n",
        "            # Request both text and image in the response\n",
        "            response_modalities=[types.Modality.TEXT, types.Modality.IMAGE]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Display any text description the model provides\n",
        "    if response.text:\n",
        "        print(\"\\033[1m\\033[4mText Output:\\033[0m\")\n",
        "        print(response.text)\n",
        "        print()\n",
        "\n",
        "    # Extract the image from the response parts\n",
        "    image_data = None\n",
        "    for part in response.candidates[0].content.parts:\n",
        "        # Try multiple possible locations for image data\n",
        "        if hasattr(part, 'inline_data') and part.inline_data:\n",
        "            if hasattr(part.inline_data, 'data'):\n",
        "                image_data = part.inline_data.data\n",
        "                break\n",
        "        elif hasattr(part, 'data'):\n",
        "            image_data = part.data\n",
        "            break\n",
        "\n",
        "    if image_data:\n",
        "        # Check if data is already bytes or needs base64 decoding\n",
        "        if isinstance(image_data, bytes):\n",
        "            image_bytes = image_data\n",
        "        else:\n",
        "            image_bytes = base64.b64decode(image_data)\n",
        "\n",
        "        # Convert bytes to PIL Image\n",
        "        image = Image.open(BytesIO(image_bytes))\n",
        "\n",
        "        # Save to file\n",
        "        image.save(\"gemini_generated.png\")\n",
        "        print(\"\\033[1m\\033[4mImage Status:\\033[0m Saved as 'gemini_generated.png'\")\n",
        "        print()\n",
        "\n",
        "        # Display the image in the notebook\n",
        "        print(\"\\033[1m\\033[4mGenerated Image:\\033[0m\")\n",
        "        display(image)\n",
        "    else:\n",
        "        print(\"\\033[1m\\033[4mNote:\\033[0m Image generation may not be available for this model.\")\n",
        "        print(\"The model returned text only. Try using the model for text-to-image tasks via the web interface.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\033[1m\\033[4mError:\\033[0m\", str(e))\n",
        "    print(\"\\nImage generation might not be available in the current API version.\")\n",
        "    print(\"The gemini-3-pro-image-preview model is primarily for image understanding, not generation.\")\n",
        "    print(\"For image generation, consider using dedicated image generation APIs like Imagen.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03c692a",
      "metadata": {
        "id": "b03c692a"
      },
      "source": [
        "## 9. Other Common Use Cases\n",
        "\n",
        "Here are some practical examples of common use cases:\n",
        "\n",
        "### 9.1 Content Summarization, Data Extraction, Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752dfe96",
      "metadata": {
        "id": "752dfe96"
      },
      "outputs": [],
      "source": [
        "# EXAMPLE 1: Content Summarization\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "print(\"\\033[1m\\033[4mEXAMPLE 1: Content Summarization\\033[0m\")\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "\n",
        "# Sample article to summarize\n",
        "article = \"\"\"\n",
        "Machine learning is transforming how we solve complex problems.\n",
        "From healthcare to finance, AI applications are improving efficiency\n",
        "and enabling new discoveries. Recent breakthroughs in deep learning\n",
        "have made systems capable of understanding language and images.\n",
        "However, challenges remain in interpretability and bias detection.\n",
        "\"\"\"\n",
        "\n",
        "# Generate a concise summary\n",
        "response = client.models.generate_content(\n",
        "    model = \"gemini-3-flash-preview\",  # Fast model for summarization\n",
        "    contents=f\"Summarize this article in 2 sentences:\\n\\n{article}\",\n",
        "    config=types.GenerateContentConfig(max_output_tokens=100, thinking_config=thinking_config,)\n",
        ")\n",
        "print(\"\\033[1m\\033[4mSummary:\\033[0m\")\n",
        "print(response.text)\n",
        "\n",
        "# EXAMPLE 2: Data Extraction\n",
        "print(\"\\n\" + \"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "print(\"\\033[1m\\033[4mEXAMPLE 2: Data Extraction\\033[0m\")\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "\n",
        "# Unstructured text containing information to extract\n",
        "text = \"John Smith, age 32, works at TechCorp. Contact: john@example.com\"\n",
        "\n",
        "# Extract structured data as JSON\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=f\"\"\"Extract structured data from this text and return as JSON:\n",
        "\n",
        "{text}\n",
        "\n",
        "Expected fields: name, age, company, email\"\"\",\n",
        "    config=types.GenerateContentConfig(max_output_tokens=200, thinking_config=thinking_config,)\n",
        ")\n",
        "print(\"\\033[1m\\033[4mExtracted Data:\\033[0m\")\n",
        "print(response.text)\n",
        "\n",
        "# EXAMPLE 3: Question Answering\n",
        "print(\"\\n\" + \"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "print(\"\\033[1m\\033[4mEXAMPLE 3: Question Answering\\033[0m\")\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "\n",
        "# Context document for answering questions\n",
        "context = \"\"\"\n",
        "The Great Wall of China is approximately 13,171 miles long.\n",
        "It was built over many centuries to protect against invasions.\n",
        "Construction began as early as the 7th century BC.\n",
        "\"\"\"\n",
        "\n",
        "# Answer a question based on the provided context\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=f\"\"\"Answer the question based on the context:\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: How long is the Great Wall of China?\"\"\",\n",
        "    config=types.GenerateContentConfig(max_output_tokens=100, thinking_config=thinking_config,)\n",
        ")\n",
        "print(\"\\033[1m\\033[4mAnswer:\\033[0m\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea3eb0f7",
      "metadata": {
        "id": "ea3eb0f7"
      },
      "source": [
        "### 9.2 Building a Mini Q&A System with Context\n",
        "\n",
        "Let's build a simple question-answering system that demonstrates retrieval-augmented generation (RAG) concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ac3ba69",
      "metadata": {
        "id": "1ac3ba69"
      },
      "outputs": [],
      "source": [
        "model = \"gemini-2.5-flash\"\n",
        "\n",
        "# Compare different prompting strategies for the same task\n",
        "task = \"Explain machine learning\"\n",
        "\n",
        "prompting_strategies = {\n",
        "    \"Vague\": \"Explain machine learning\",\n",
        "\n",
        "    \"Specific with audience\": \"Explain machine learning to a 10-year-old child using simple analogies\",\n",
        "\n",
        "    \"Structured output\": \"\"\"Explain machine learning with the following structure:\n",
        "    1. Definition (1 sentence)\n",
        "    2. How it works (2 sentences)\n",
        "    3. Real-world example (1 sentence)\n",
        "    4. Why it matters (1 sentence)\"\"\",\n",
        "\n",
        "    \"With constraints\": \"\"\"Explain machine learning in exactly 4 sentences.\n",
        "    Use the analogy of teaching a pet a trick.\n",
        "    Avoid technical jargon.\"\"\"\n",
        "}\n",
        "\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "print(\"\\033[1m\\033[4mPROMPT ENGINEERING: Strategy Comparison\\033[0m\")\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "\n",
        "for strategy_name, prompt in prompting_strategies.items():\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=prompt,\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.5,\n",
        "            max_output_tokens=100,\n",
        "            thinking_config=thinking_config,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"\\033[1m\\033[4mSTRATEGY: {strategy_name}\\033[0m\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"\\033[1mPrompt:\\033[0m {prompt[:80]}...\" if len(prompt) > 80 else f\"\\033[1mPrompt:\\033[0m {prompt}\")\n",
        "    print(f\"\\n\\033[1mResponse:\\033[0m\")\n",
        "    print(response.text)\n",
        "    print()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Notice how different prompting strategies yield different results!\")\n",
        "print(\"Key lessons:\")\n",
        "print(\"- Specificity improves relevance\")\n",
        "print(\"- Audience targeting adjusts complexity\")\n",
        "print(\"- Structure ensures completeness\")\n",
        "print(\"- Constraints control format and style\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "734a4742",
      "metadata": {
        "id": "734a4742"
      },
      "source": [
        "### 9.3 Chain-of-Thought Reasoning\n",
        "\n",
        "Demonstrates how prompting the model to think step-by-step improves accuracy on reasoning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b5a1fb1",
      "metadata": {
        "id": "7b5a1fb1"
      },
      "outputs": [],
      "source": [
        "model = \"gemini-2.5-flash\"\n",
        "\n",
        "# Complex reasoning problem\n",
        "problem = \"\"\"A farmer has 17 sheep, and all but 9 die. How many are left?\n",
        "\n",
        "Let's approach this step-by-step:\n",
        "1. Carefully read and understand what \"all but 9\" means\n",
        "2. Break down the problem\n",
        "3. Calculate the answer\n",
        "4. Verify our reasoning\n",
        "\n",
        "Think through this carefully before answering.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model,\n",
        "    contents=problem,\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.3,\n",
        "        max_output_tokens=400,\n",
        "        thinking_config=thinking_config,\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "print(\"\\033[1m\\033[4mCHAIN-OF-THOUGHT REASONING\\033[0m\")\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "print(f\"\\033[1mProblem:\\033[0m {problem.split('Let')[0].strip()}\")\n",
        "print()\n",
        "print(\"\\033[1mModel's Reasoning:\\033[0m\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60b46272",
      "metadata": {
        "id": "60b46272"
      },
      "source": [
        "### 9.4 Creative Writing: Story Generation with Style Transfer\n",
        "\n",
        "LLMs can generate creative content in various styles by learning from the patterns and structures in their training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd198213",
      "metadata": {
        "id": "bd198213"
      },
      "outputs": [],
      "source": [
        "model = \"gemini-2.5-flash\"\n",
        "\n",
        "# Story prompt with style variations\n",
        "base_story = \"A programmer discovers their code has become sentient.\"\n",
        "\n",
        "styles = [\n",
        "    \"Shakespearean drama\",\n",
        "    \"Hard science fiction\",\n",
        "    \"Children's picture book\"\n",
        "]\n",
        "\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "print(\"\\033[1m\\033[4mCREATIVE WRITING: Style Transfer\\033[0m\")\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "print(f\"\\033[1mBase Story Premise:\\033[0m {base_story}\")\n",
        "print()\n",
        "\n",
        "for idx, style in enumerate(styles, 1):\n",
        "    prompt = f\"\"\"Write a short story (3-4 paragraphs) based on this premise:\n",
        "\"{base_story}\"\n",
        "\n",
        "Write it in the style of: {style}\n",
        "\n",
        "Make it engaging and authentic to that style.\"\"\"\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=prompt,\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.9,  # High temperature for creativity\n",
        "            max_output_tokens=500,\n",
        "            thinking_config=thinking_config,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"\\033[1m\\033[4mSTYLE {idx}: {style.upper()}\\033[0m\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(response.text)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4655b7f",
      "metadata": {
        "id": "f4655b7f"
      },
      "source": [
        "### 9.5 Text Classification and Sentiment Analysis\n",
        "\n",
        "LLMs excel at **zero-shot** and **few-shot learning**, where they can perform tasks without explicit training, leveraging their broad pre-training.\n",
        "\n",
        "**Zero-shot Learning**: Performing a task with just a description, no examples.\n",
        "**Few-shot Learning**: Performing a task with a handful of examples in the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "defb6ee7",
      "metadata": {
        "id": "defb6ee7"
      },
      "outputs": [],
      "source": [
        "model = \"gemini-2.5-flash\"\n",
        "\n",
        "# Zero-shot sentiment analysis\n",
        "reviews = [\n",
        "    \"This product exceeded my expectations! Absolutely love it.\",\n",
        "    \"Terrible quality, broke after one day. Do not buy.\",\n",
        "    \"It's okay, nothing special but gets the job done.\",\n",
        "    \"Best purchase I've made this year! Highly recommend.\"\n",
        "]\n",
        "\n",
        "prompt = \"\"\"Analyze the sentiment of these customer reviews and classify each as POSITIVE, NEGATIVE, or NEUTRAL.\n",
        "Also provide a confidence score (0-1) and a brief reason.\n",
        "\n",
        "Format as JSON array with fields: review, sentiment, confidence, reason\n",
        "\n",
        "Reviews:\n",
        "\"\"\"\n",
        "\n",
        "for i, review in enumerate(reviews, 1):\n",
        "    prompt += f\"{i}. {review}\\n\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model,\n",
        "    contents=prompt,\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,  # Low temperature for consistent classification\n",
        "        max_output_tokens=800,\n",
        "        thinking_config=thinking_config,\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "print(\"\\033[1m\\033[4mSentiment Analysis Results:\\033[0m\")\n",
        "print(\"\\033[1m\" + \"=\"*70 + \"\\033[0m\")\n",
        "print(response.text)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}